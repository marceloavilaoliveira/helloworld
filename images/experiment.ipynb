{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador de tickets do Jira - Experimento\n",
    "\n",
    "Este é um componente que utiliza a biblioteca [auto-sklearn](https://github.com/automl/auto-sklearn) para obter um ou mais modelos classificadores já otimizados. <br>\n",
    "O auto-sklearn é um kit de ferramentas de machine learning automatizado e um substituto para [estimator](https://scikit-learn.org/stable/glossary.html#term-estimators) do [scikit-learn](https://scikit-learn.org/stable/).\n",
    "\n",
    "Este notebook apresenta:\n",
    "- como usar o [SDK](https://platiagro.github.io/sdk/) para carregar datasets, salvar modelos e outros artefatos.\n",
    "- como declarar parâmetros e usá-los para criar componentes reutilizáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaração de parâmetros e hiperparâmetros\n",
    "\n",
    "Declare parâmetros com o botão <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAABhWlDQ1BJQ0MgcHJvZmlsZQAAKJF9kT1Iw0AcxV9TtaIVBzuIOASpThb8QhylikWwUNoKrTqYXPohNGlIUlwcBdeCgx+LVQcXZ10dXAVB8APEydFJ0UVK/F9SaBHjwXE/3t173L0DhFqJqWbbGKBqlpGMRcVMdkUMvKID3QhiCOMSM/V4aiENz/F1Dx9f7yI8y/vcn6NHyZkM8InEs0w3LOJ14ulNS+e8TxxiRUkhPiceNeiCxI9cl11+41xwWOCZISOdnCMOEYuFFpZbmBUNlXiKOKyoGuULGZcVzluc1VKFNe7JXxjMacsprtMcRAyLiCMBETIq2EAJFiK0aqSYSNJ+1MM/4PgT5JLJtQFGjnmUoUJy/OB/8LtbMz854SYFo0D7i21/DAOBXaBete3vY9uunwD+Z+BKa/rLNWDmk/RqUwsfAb3bwMV1U5P3gMsdoP9JlwzJkfw0hXweeD+jb8oCfbdA16rbW2Mfpw9AmrpaugEODoGRAmWveby7s7W3f880+vsBocZyukMJsmwAAAAGYktHRAD/AP8A/6C9p5MAAAAJcEhZcwAADdcAAA3XAUIom3gAAAAHdElNRQfkBgsMIwnXL7c0AAACDUlEQVQ4y92UP4gTQRTGf29zJxhJZ2NxbMBKziYWlmJ/ile44Nlkd+dIYWFzItiNgoIEtFaTzF5Ac/inE/urtLWxsMqmUOwCEpt1Zmw2xxKi53XitPO9H9978+aDf/3IUQvSNG0450Yi0jXG7C/eB0cFeu9viciGiDyNoqh2KFBrHSilWstgnU7nFLBTgl+ur6/7PwK11kGe5z3n3Hul1MaiuCgKDZwALHA7z/Oe1jpYCtRaB+PxuA8kQM1aW68Kt7e3zwBp6a5b1ibj8bhfhQYVZwMRiQHrvW9nWfaqCrTWPgRWvPdvsiy7IyLXgEJE4slk8nw+T5nDgDbwE9gyxryuwpRSF5xz+0BhrT07HA4/AyRJchUYASvAbhiGaRVWLIMBYq3tAojIszkMoNRulbXtPM8HwV/sXSQi54HvQRDcO0wfhGGYArvAKjAq2wAgiqJj3vsHpbtur9f7Vi2utLx60LLW2hljEuBJOYu9OI6vAzQajRvAaeBLURSPlsBelA+VhWGYaq3dwaZvbm6+m06noYicE5ErrVbrK3AXqHvvd4bD4Ye5No7jSERGwKr3Pms2m0pr7Rb30DWbTQWYcnFvAieBT7PZbFB1V6vVfpQaU4UtDQetdTCZTC557/eA48BlY8zbRZ1SqrW2tvaxCvtt2iRJ0i9/xb4x5uJRwmNlaaaJ3AfqIvKY/+78Av++6uiSZhYMAAAAAElFTkSuQmCC\" /> na barra de ferramentas.<br>\n",
    "O parâmetro `dataset` identifica os conjuntos de dados. Você pode importar arquivos de dataset com o botão <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAABhWlDQ1BJQ0MgcHJvZmlsZQAAKJF9kT1Iw0AcxV9TtaIVBzuIOASpThb8QhylikWwUNoKrTqYXPohNGlIUlwcBdeCgx+LVQcXZ10dXAVB8APEydFJ0UVK/F9SaBHjwXE/3t173L0DhFqJqWbbGKBqlpGMRcVMdkUMvKID3QhiCOMSM/V4aiENz/F1Dx9f7yI8y/vcn6NHyZkM8InEs0w3LOJ14ulNS+e8TxxiRUkhPiceNeiCxI9cl11+41xwWOCZISOdnCMOEYuFFpZbmBUNlXiKOKyoGuULGZcVzluc1VKFNe7JXxjMacsprtMcRAyLiCMBETIq2EAJFiK0aqSYSNJ+1MM/4PgT5JLJtQFGjnmUoUJy/OB/8LtbMz854SYFo0D7i21/DAOBXaBete3vY9uunwD+Z+BKa/rLNWDmk/RqUwsfAb3bwMV1U5P3gMsdoP9JlwzJkfw0hXweeD+jb8oCfbdA16rbW2Mfpw9AmrpaugEODoGRAmWveby7s7W3f880+vsBocZyukMJsmwAAAAGYktHRAD/AP8A/6C9p5MAAAAJcEhZcwAADdcAAA3XAUIom3gAAAAHdElNRQfkBgsOBy6ASTeXAAAC/0lEQVQ4y5WUT2gcdRTHP29m99B23Uiq6dZisgoWCxVJW0oL9dqLfyhCvGWY2YUBI95MsXgwFISirQcLhS5hfgk5CF3wJIhFI7aHNsL2VFZFik1jS1qkiZKdTTKZ3/MyDWuz0fQLc/m99/vMvDfv+4RMlUrlkKqeAAaBAWAP8DSgwJ/AXRG5rao/WWsvTU5O3qKLBMD3fSMiPluXFZEPoyj67PGAMzw83PeEMABHVT/oGpiamnoAmCcEWhH5tFsgF4bh9oWFhfeKxeJ5a+0JVT0oImWgBPQCKfAQuAvcBq67rltX1b+6ApMkKRcKhe9V9QLwbavV+qRer692Sx4ZGSnEcXw0TdP3gSrQswGYz+d/S5IkVtXTwOlCoZAGQXAfmAdagAvsAErtdnuXiDy6+023l7qNRsMODg5+CawBzwB9wFPA7mx8ns/KL2Tl3xCRz5eWlkabzebahrHxPG+v4zgnc7ncufHx8Z+Hhoa29fT0lNM03Q30ikiqqg+ttX/EcTy3WTvWgdVqtddaOw/kgXvADHBHROZVNRaRvKruUNU+EdkPfGWM+WJTYOaSt1T1LPDS/4zLWWPMaLVaPWytrYvIaBRFl/4F9H2/JCKvGmMu+76/X0QOqGoZKDmOs1NV28AicMsYc97zvFdc1/0hG6kEeNsY83UnsCwivwM3VfU7YEZE7lhr74tIK8tbnJiYWPY8b6/ruleAXR0ftQy8boyZXi85CIIICDYpc2ZgYODY3NzcHmvt1eyvP64lETkeRdE1yZyixWLx5U2c8q4x5mIQBE1g33/0d3FlZeXFR06ZttZesNZejuO4q1NE5CPgWVV9E3ij47wB1IDlJEn+ljAM86urq7+KyAtZTgqsO0VV247jnOnv7/9xbGzMViqVMVX9uANYj6LonfVtU6vVkjRNj6jqGeCXzGrPAQeA10TkuKpOz87ONrayhnIA2Qo7BZwKw3B7kiRloKSqO13Xja21C47jPNgysFO1Wi0GmtmzQap6DWgD24A1Vb3SGf8Hfstmz1CuXEIAAAAASUVORK5CYII=\" /> na barra de ferramentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset = \"jira-2.csv\" #@param {type:\"string\"}\n",
    "target = \"issuetype\" #@param {type:\"feature\", label:\"Atributo alvo\", description:\"Seu modelo será treinado para prever os valores do alvo.\"}\n",
    "text = \"description\" #@param \n",
    "\n",
    "# selected features to perform the model\n",
    "filter_type = \"incluir\" #@param [\"incluir\",\"remover\"] {type:\"string\",multiple:false,label:\"Modo de seleção das features\",description:\"Se deseja informar quais features deseja incluir no modelo, selecione a opção 'incluir'. Caso deseje informar as features que não devem ser utilizadas, selecione 'remover'. \"}\n",
    "model_features = [\"text\"] #@param {type:\"feature\",multiple:true,label:\"Features para incluir/remover no modelo\",description:\"Seu modelo será feito considerando apenas as features selecionadas. Caso nada seja especificado, todas as features serão utilizadas\"}\n",
    "\n",
    "# features to apply One Hot Encoder\n",
    "one_hot_features = \"\" #@param {type:\"feature\", multiple:true, label:\"Features para fazer codificação one-hot\", description:\"Seu modelo utilizará a codificação one-hot para as features selecionadas. As demais features categóricas serão codificadas utilizando a codificação ordinal.\"}\n",
    "\n",
    "# hyperparameters\n",
    "time_left_for_this_task = 60 #@param {type:\"integer\", label:\"Tempo máximo de busca\", description:\"Limite de tempo (em segundos) para a procura de modelos apropriados.\"}\n",
    "per_run_time_limit = 60 #@param {type:\"integer\", label:\"Tempo máximo para ajuste de modelo\", description:\"Limite de tempo (em segundos), para uma única chamada, para ajuste de um modelo de Machine Learning.\"}\n",
    "ensemble_size = 50 #@param {type:\"integer\", label:\"Ensemble Learning\", description:\"Número de modelos adicionados ao conjunto criado pela seleção do Ensemble das bibliotecas de modelos.\"}\n",
    "model_name = 'neuralmind/bert-base-portuguese-cased'  #@param [\"bert-base-uncased\", \"roberta-base\",\"neuralmind/bert-base-portuguese-cased\"] {type:\"string\"}\n",
    "batch_size =  4#@param {type:\"integer\"}\n",
    "max_epochs = 3 #@param {type:\"integer\"}\n",
    "accumulate_grad_batches = 16  #@param {type:\"integer\"}\n",
    "max_length = 512  #@param {type:\"integer\"}\n",
    "learning_rate = 2e-5  #@param {type:\"number\"}\n",
    "seed = 7  #@param {type:\"integer\"}\n",
    "\n",
    "# predict method\n",
    "method = \"predict_proba\" #@param [\"predict_proba\", \"predict\"] {type:\"string\", label:\"Método de Predição\", description:\"Se optar por 'predict_proba', o método de predição será a probabilidade estimada de cada classe, já o 'predict' prediz a qual classe pertence.\"} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaçao dos pacotes externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning==0.8.4 --quiet\n",
    "! pip install transformers --quiet\n",
    "! pip install ftfy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparatiovos para Modelos de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo GPU como padrão e verificando status de hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import torch\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(dev)\n",
    "if dev == \"cpu\":\n",
    "    print(f\"number of CPU cores: {cpu_count()}\")\n",
    "else:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}, number of CPU cores: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciando Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel,BertForSequenceClassification, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "hyperparms = {'model_name':model_name,'tokenizer':tokenizer,'learning_rate':learning_rate,'batch_size':batch_size,'max_epochs':max_epochs,'accumulate_grad_batches':accumulate_grad_batches,'max_length':max_length} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impedir Excesso de logs nos modelos de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixar semente de pesos aleatporios para replicabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorator que auxilia para impedir quebra de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import traceback\n",
    "import psutil\n",
    "def gpu_mem_restore(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = sys.exc_info()\n",
    "            traceback.clear_frames(tb)\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluindo informações adicionais na barra de carregamento dos modelos do Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso ao conjunto de dados\n",
    "\n",
    "O conjunto de dados utilizado nesta etapa será o mesmo carregado através da plataforma.<br>\n",
    "O tipo da variável retornada depende do arquivo de origem:\n",
    "- [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) para CSV e compressed CSV: .csv .csv.zip .csv.gz .csv.bz2 .csv.xz\n",
    "- [Binary IO stream](https://docs.python.org/3/library/io.html#binary-i-o) para outros tipos de arquivo: .jpg .wav .zip .h5 .parquet etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>issuetype</th>\n",
       "      <th>summary</th>\n",
       "      <th>components</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HD-82125</td>\n",
       "      <td>Requisição</td>\n",
       "      <td>Jira e confluence não acessar sem VPN e VPN do...</td>\n",
       "      <td>VPN</td>\n",
       "      <td>Jira e confluence não acessar sem VPN e VPN do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HD-82111</td>\n",
       "      <td>Requisição</td>\n",
       "      <td>DT para levar monitor de computador DELL para ...</td>\n",
       "      <td>Partes/peças</td>\n",
       "      <td>Boa tarde, tudo bem?\\n\\nGostaria de solicitar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HD-82104</td>\n",
       "      <td>Requisição</td>\n",
       "      <td>Acesso ao Novo SVN</td>\n",
       "      <td>Jira</td>\n",
       "      <td>Gustavo, boa tarde!\\n\\nPor gentileza, adiciona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HD-82097</td>\n",
       "      <td>Requisição</td>\n",
       "      <td>Instalação de softwares - Troca de máquina</td>\n",
       "      <td>Software Windows</td>\n",
       "      <td>Devido ao trabalho remoto, está inviável traba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HD-82092</td>\n",
       "      <td>Incidente</td>\n",
       "      <td>Solicitação de perfil de administrador via aut...</td>\n",
       "      <td>Autoatendimento</td>\n",
       "      <td>Estou conectado na nova VPN do CPQD (via Forti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0   issuetype                                            summary  \\\n",
       "0   HD-82125  Requisição  Jira e confluence não acessar sem VPN e VPN do...   \n",
       "1   HD-82111  Requisição  DT para levar monitor de computador DELL para ...   \n",
       "2   HD-82104  Requisição                                 Acesso ao Novo SVN   \n",
       "3   HD-82097  Requisição         Instalação de softwares - Troca de máquina   \n",
       "4   HD-82092   Incidente  Solicitação de perfil de administrador via aut...   \n",
       "\n",
       "         components                                        description  \n",
       "0               VPN  Jira e confluence não acessar sem VPN e VPN do...  \n",
       "1      Partes/peças  Boa tarde, tudo bem?\\n\\nGostaria de solicitar,...  \n",
       "2              Jira  Gustavo, boa tarde!\\n\\nPor gentileza, adiciona...  \n",
       "3  Software Windows  Devido ao trabalho remoto, está inviável traba...  \n",
       "4   Autoatendimento  Estou conectado na nova VPN do CPQD (via Forti...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'/tmp/data/{dataset}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removendo as classes desnecessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[text, target]]\n",
    "df = df.rename(columns={text: \"text\", target: \"target\"})\n",
    "text, target = \"text\",\"target\"\n",
    "df = df[(df.target != 'Epic') & (df.target != 'Demanda') & (df.target != 'Bug') & (df.target != 'Sub-task')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso aos metadados do conjunto de dados\n",
    "\n",
    "Utiliza a função `stat_dataset` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para carregar metadados. <br>\n",
    "Por exemplo, arquivos CSV possuem `metadata['featuretypes']` para cada coluna no conjunto de dados (ex: categorical, numerical, or datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "from platiagro import save_dataset\n",
    "\n",
    "save_dataset(name=dataset, df=df)\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "columns = df.columns.to_numpy()\n",
    "featuretypes = np.array(featuretypes)\n",
    "target_index = np.argwhere(columns == target)\n",
    "columns = np.delete(columns, target_index)\n",
    "featuretypes = np.delete(featuretypes, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de linhas com valores faltantes no atributo alvo\n",
    "\n",
    "Filtragem de elementos do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_filter(original_cell):\n",
    "    cell = re.sub('{.*?}', ' ', original_cell) #Remoção das tags de color:{#000000}\n",
    "    cell = re.sub('<.*?>', ' ', cell) #Remoção das tags de espaço ou qualquer outra que aparecer\n",
    "    cell = re.sub(r'http\\S+', '', cell) #Remoção de endereços web\n",
    "    cell = re.sub(\"/\\r\\n|\\n|\\r|\", \"\",cell);#Remoção de line breaks por \\n e \\r\n",
    "    cell = re.sub(' +', ' ',cell) #Transformação de multiplos espaços em um único espaço\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso haja linhas em que o atributo alvo contenha valores faltantes, é feita a remoção dos casos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dropna(subset = [target],inplace=True)\n",
    "df=df.dropna()\n",
    "df[text] = df[text].apply(lambda x: text_filter(x))\n",
    "y = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem das features \n",
    "\n",
    "Seleciona apenas as features que foram declaradas no parâmetro model_features. Se nenhuma feature for especificada, todo o conjunto de dados será utilizado para a modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_type == 'incluir':\n",
    "    if len(model_features) >= 1:\n",
    "        columns_index = (np.where(np.isin(columns,model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = columns[columns_index]\n",
    "        featuretypes = featuretypes[columns_index]\n",
    "    else:\n",
    "        columns_to_filter = columns\n",
    "else:\n",
    "    if len(model_features) >= 1:\n",
    "        columns_index = (np.where(np.isin(columns,model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = np.delete(columns,columns_index)\n",
    "        featuretypes = np.delete(featuretypes,columns_index)\n",
    "    else:\n",
    "        columns_to_filter = columns\n",
    "\n",
    "# keep the features selected\n",
    "df_model = df[columns_to_filter]\n",
    "X = df_model.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codifica labels do atributo alvo\n",
    "\n",
    "As labels do atributo alvo são convertidos em números inteiros ordinais com valor entre 0 e n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset em subconjuntos de treino e teste\n",
    "\n",
    "Subconjunto de treino: amostra de dados usada para treinar o modelo.<br>\n",
    "Subconjunto de teste: amostra de dados usada para fornecer uma avaliação imparcial do treinamento do modelo no subconjunto de dados de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  train_size=0.9,random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração das features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.featuretypes import NUMERICAL\n",
    "\n",
    "# Selects the indexes of numerical and non-numerical features\n",
    "numerical_indexes = np.where(featuretypes == NUMERICAL)[0]\n",
    "non_numerical_indexes = np.where(~(featuretypes == NUMERICAL))[0]\n",
    "\n",
    "# Selects non-numerical features to apply ordinal encoder or one-hot encoder\n",
    "one_hot_features = np.asarray(one_hot_features)\n",
    "non_numerical_indexes_one_hot = np.where(~(featuretypes == NUMERICAL) & np.isin(columns_to_filter,one_hot_features))[0]\n",
    "non_numerical_indexes_ordinal = np.where(~(featuretypes == NUMERICAL) & ~(np.isin(columns_to_filter,one_hot_features)))[0]\n",
    "\n",
    "# After the step handle_missing_values, \n",
    "# numerical features are grouped in the beggining of the array\n",
    "numerical_indexes_after_handle_missing_values = \\\n",
    "    np.arange(len(numerical_indexes))\n",
    "non_numerical_indexes_after_handle_missing_values = \\\n",
    "    np.arange(len(numerical_indexes), len(featuretypes))\n",
    "one_hot_indexes_after_handle_missing_values = non_numerical_indexes_after_handle_missing_values[np.where(np.isin(non_numerical_indexes,non_numerical_indexes_one_hot))[0]]         \n",
    "ordinal_indexes_after_handle_missing_values = non_numerical_indexes_after_handle_missing_values[np.where(np.isin(non_numerical_indexes,non_numerical_indexes_ordinal))[0]]                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import ftfy\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts: List[str],target: List[int],tokenizer,max_length):\n",
    "        self.texts = texts\n",
    "        self.target =  torch.tensor(target).type(torch.long)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoder_plus = self.encoder_plus(ftfy.fix_text(text[0]))\n",
    "        token_ids = encoder_plus['input_ids']\n",
    "        attention_mask = encoder_plus['attention_mask']\n",
    "        token_type_ids = encoder_plus['token_type_ids']\n",
    "        #tranformacao para tensor\n",
    "        token_ids = torch.tensor(token_ids).type(torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask).type(torch.long)\n",
    "        token_type_ids = torch.tensor(token_type_ids).type(torch.long)\n",
    "\n",
    "        return token_ids, attention_mask, token_type_ids, self.target[idx]\n",
    "  \n",
    "    #already truncate and paded\n",
    "    def encoder_plus(self,text):\n",
    "        return self.tokenizer.encode_plus(text,\n",
    "                                  max_length = self.max_length,\n",
    "                                  pad_to_max_length=True,\n",
    "                                  truncation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando o Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aumentar tablespace dos bancos SAFRMIG7_SCH e SAF*_MIG4.Ao tentar criar a partição no banco SAFRMIG7_SCH, foi apresentado um erro informando que não era possível estender a tabela sys.indpart. Vide evidência em anexo.Solicito que o tamanho da tablespace seja expandido tanto para o banco SAFRMIG7_SCH quanto para os do SAF*_MIG4.Usuário: SAFRMIG7_SCHHost: vmbd7.cpqd.com.brPorta: 1521Nome do serviço: bd710i1Usuário: SAF*_MIG4Host: vmbd50.cpqd.com.brPorta: 1521Nome do serviço: bd511i3Obs.: O Aijalom Correa já está ciente do problema. Favor, encaminhar ao mesmo.']\n",
      "1\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "--------------------------------------------------\n",
      "token_ids:\n",
      " tensor([[  101,   115,   115,  ...,     0,     0,     0],\n",
      "        [  101,  8399,   644,  ...,     0,     0,     0],\n",
      "        [  101,  4407, 16682,  ...,     0,     0,     0],\n",
      "        [  101,  7723,  1373,  ...,     0,     0,     0]])\n",
      "token_type_ids:\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "attention_mask:\n",
      " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "target:\n",
      " tensor([1, 1, 1, 0])\n",
      "token_ids.shape: torch.Size([4, 512])\n",
      "token_type_ids.shape: torch.Size([4, 512])\n",
      "attention_mask.shape: torch.Size([4, 512])\n",
      "target.shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_debug = X_train[:10]\n",
    "y_debug = y_train[:10]\n",
    "\n",
    "print(X_debug[0])\n",
    "print(y_debug[0])\n",
    "print(type(X_debug))\n",
    "print(type(y_debug))\n",
    "print(50*'-')\n",
    "\n",
    "dataset_debug = MyDataset(\n",
    "    texts=X_debug,\n",
    "    target=y_debug,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length)\n",
    "\n",
    "\n",
    "dataloader_debug = DataLoader(dataset_debug, batch_size=batch_size, shuffle=True, num_workers=cpu_count())\n",
    "token_ids, attention_mask, token_type_ids, target = next(iter(dataloader_debug))\n",
    "\n",
    "print('token_ids:\\n', token_ids)\n",
    "print('token_type_ids:\\n', token_type_ids)\n",
    "print('attention_mask:\\n', attention_mask)\n",
    "print('target:\\n', target)\n",
    "\n",
    "print('token_ids.shape:', token_ids.shape)\n",
    "print('token_type_ids.shape:', token_type_ids.shape)\n",
    "print('attention_mask.shape:', attention_mask.shape)\n",
    "print('target.shape:', target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe que define o modelo do pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class BertFinetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 hyperparms,\n",
    "                 all_data,\n",
    "                 criterion = torch.nn.CrossEntropyLoss(),\n",
    "                 overfit=False,\n",
    "                 sampler = False):\n",
    "      \n",
    "\n",
    "        super(BertFinetuner, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(hyperparms['model_name'])\n",
    "        self.predict_proba  =  torch.nn.Softmax(dim=1)\n",
    "        #---------- Hyperparametros\n",
    "        \n",
    "        self.tokenizer = hyperparms['tokenizer']\n",
    "        self.learning_rate = hyperparms['learning_rate']\n",
    "        self.batch_size = hyperparms['batch_size']\n",
    "        self.max_length = hyperparms['max_length']\n",
    "        self.overfit = overfit\n",
    "        self.training = False\n",
    "\n",
    "        #---------- Carregamento datasets (Para eu poder variar self.max_length)\n",
    "        self.sampler = sampler\n",
    "        self.targets_sampler = all_data[1]\n",
    "        if overfit:\n",
    "            self.train_dataset = MyDataset(all_data[0], all_data[1],self.tokenizer,self.max_length)\n",
    "            self.test_dataset =   MyDataset(all_data[0], all_data[1],self.tokenizer,self.max_length)\n",
    "        else:\n",
    "            self.train_dataset = MyDataset(all_data[0], all_data[1],self.tokenizer,self.max_length)\n",
    "            self.test_dataset = MyDataset(all_data[2], all_data[3],self.tokenizer,self.max_length)\n",
    "        \n",
    "        \n",
    "        #---------- Loss Function\n",
    "        self.loss_funtion = criterion\n",
    "\n",
    "        #---------- Datafame de comparação para o teste\n",
    "        self.df_test = pd.DataFrame(columns=['ORIGINAL_TARGET','ORIGINAL_CODE','PREDICTED_TARGET','PREDICTED_CODE','INC_PROBA','REQ_PROBA'])\n",
    "        \n",
    "        self.classification_specific = False\n",
    "\n",
    "\n",
    "        #---------- Englobamentoda rede para classificação(se necessário)\n",
    "        if self.classification_specific:\n",
    "            pass\n",
    "        else:\n",
    "            self.num_classes = 2\n",
    "            self.classification_layer = torch.nn.Linear(self.model.config.hidden_size, self.num_classes)\n",
    "        \n",
    "    def infer(self,ticket):\n",
    "        tok = self.tokenizer.encode_plus(ticket, return_tensors='pt')\n",
    "        logits = self.forward(tok['input_ids'].to(device), tok['attention_mask'].to(device),tok['token_type_ids'].to(device))\n",
    "        _, predicted_codes = torch.max(logits, dim=1)\n",
    "        return label_encoder.inverse_transform(predicted_codes.data.cpu().numpy())\n",
    "\n",
    "    def infer_probablities(self,ticket):\n",
    "        tok = self.tokenizer.encode_plus(ticket, return_tensors='pt')\n",
    "        logits = self.forward(tok['input_ids'].to(device), tok['attention_mask'].to(device),tok['token_type_ids'].to(device))\n",
    "        return self.predict_proba(logits).data.cpu().numpy()\n",
    "\n",
    "    def forward(self, token_ids, attention_mask, token_type_ids=None, original_codes=None):\n",
    "\n",
    "        if self.classification_specific:\n",
    "            outputs = self.model(token_ids,labels=original_codes,attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs[1]\n",
    "        else:\n",
    "            outputs = self.model(token_ids,attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            hidden_states = outputs[0]\n",
    "            cls_vector = hidden_states[:, 0, :]\n",
    "            logits = self.classification_layer(cls_vector)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        token_ids, attention_mask, token_type_ids, original_codes = batch\n",
    "         \n",
    "        # fwd\n",
    "        y_hat = self.forward(token_ids, attention_mask, token_type_ids, original_codes)\n",
    "        \n",
    "        # loss\n",
    "        loss = self.loss_funtion(y_hat, original_codes)\n",
    "        \n",
    "        # What to log\n",
    "        tensorboard_logs = {'loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        token_ids, attention_mask, token_type_ids, original_codes = batch\n",
    "         \n",
    "        # fwd\n",
    "        y_hat = self.forward(token_ids, attention_mask, token_type_ids, original_codes)\n",
    "        \n",
    "        # Calculate acc\n",
    "        acc = self.get_acc(y_hat, original_codes)\n",
    "\n",
    "        #constructing dataframe\n",
    "        _, predicted_codes = torch.max(y_hat, dim=1)\n",
    "        predicted_targets = label_encoder.inverse_transform(predicted_codes.data.cpu().numpy())\n",
    "        original_targets = label_encoder.inverse_transform(original_codes.data.cpu().numpy())\n",
    "        classes_probabilities = self.predict_proba(y_hat).data.cpu().numpy()        \n",
    "        for original_target,original_code,predicted_target,predicted_code,classes_probability in zip(original_targets,original_codes,predicted_targets,predicted_codes,classes_probabilities):\n",
    "            self.df_test = self.df_test.append(pd.Series([original_target,int(original_code),predicted_target,int(predicted_code),classes_probability[0],classes_probability[1]], index=self.df_test.columns ), ignore_index=True)\n",
    "            \n",
    "        \n",
    "        return {'test_acc_batch': acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        if not outputs: return {}\n",
    "\n",
    "        avg_test_acc = torch.stack([x['test_acc_batch'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'avg_test_acc': avg_test_acc}\n",
    "\n",
    "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.learning_rate, eps=1e-08)\n",
    "    \n",
    "    def get_acc(self,y_hat,original_codes):\n",
    "        _, y_hat = torch.max(y_hat, dim=1)\n",
    "        val_acc = accuracy_score(y_hat.cpu(), original_codes.cpu())\n",
    "        return torch.tensor(val_acc)\n",
    "    \n",
    "    \n",
    "    #@gpu_mem_restore\n",
    "    def train_dataloader(self):\n",
    "        if self.sampler:\n",
    "          # shuffle = False if self.overfit else True\n",
    "            targets = []\n",
    "            for target in self.targets_sampler:\n",
    "                targets.append(target)\n",
    "            targets = torch.tensor(targets).type(torch.long)\n",
    "            # Compute samples weight (each sample should get its own weight)\n",
    "            class_sample_count = torch.tensor( [(targets == t).sum() for t in torch.unique(targets, sorted=True)])\n",
    "            weight = 1. / class_sample_count.float()\n",
    "            samples_weight = torch.tensor([weight[t] for t in targets])\n",
    "\n",
    "            # Create sampler, dataset, loader\n",
    "            sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "            shuffle = False\n",
    "        else:\n",
    "            shuffle = True\n",
    "            sampler = None\n",
    "\n",
    "        return DataLoader(self.train_dataset, sampler = sampler, batch_size=self.batch_size, shuffle=shuffle,num_workers=cpu_count())\n",
    "    \n",
    "    \n",
    "    #@gpu_mem_restore\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,shuffle=False, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando pesos da Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cp gs://platiagro/Jira_Tickets_Classification/jira_classification_epoch0ckpt.zip  /home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774\n",
    "# ! gsutil cp gs://platiagro/Jira_Tickets_Classification/jira_classification_epoch1ckpt.zip  /home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774\n",
    "# ! gsutil cp gs://platiagro/Jira_Tickets_Classification/jira_classification_epoch2ckpt.zip  /home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraindo os pesos do arquivo zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip \"/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/jira_classification_epoch0ckpt.zip\" -d \"/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/\"\n",
    "# !unzip \"/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/jira_classification_epoch1ckpt.zip\" -d \"/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/\"\n",
    "# !unzip \"/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/jira_classification_epoch2ckpt.zip\" -d \"/home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in /home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774: ['jira.csv', 'jira_classification_epoch1ckpt.zip', '.ipynb_checkpoints', 'Experiment.ipynb', 'jira_classification_epoch0ckpt.zip', 'lightning_logs', 'epoch=2.ckpt', 'Deployment.ipynb', 'epoch=1.ckpt', 'epoch=0.ckpt', 'jira_classification_epoch2ckpt.zip']\n",
      "Saving checkpoints to /home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774\n",
      "Restoring checkpoint: /home/jovyan/components/d25c17c8-f3f5-4e8a-a28c-8df4c48f9774/epoch=2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                 | Type             | Params\n",
      "----------------------------------------------------------\n",
      "0 | model                | BertModel        | 108 M \n",
      "1 | predict_proba        | Softmax          | 0     \n",
      "2 | loss_funtion         | CrossEntropyLoss | 0     \n",
      "3 | classification_layer | Linear           | 1 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423112346f144cce8dd0bf7a00ae378f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action              \t|  Mean duration (s)\t|  Total time (s) \n",
      "-----------------------------------------------------------------\n",
      "on_train_start      \t|  0.11521        \t|  0.11521        \n",
      "on_train_end        \t|  0.0030123      \t|  0.0030123      \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_path = cwd + '/epoch=2.ckpt'\n",
    "checkpoint_dir = os.path.dirname(os.path.abspath(checkpoint_path))\n",
    "print(f'Files in {checkpoint_dir}: {os.listdir(checkpoint_dir)}')\n",
    "print(f'Saving checkpoints to {checkpoint_dir}')\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_dir, save_top_k=-1,monitor=\"val_acc\",mode=\"max\")  # Keeps all checkpoints.\n",
    "\n",
    "resume_from_checkpoint = None\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f'Restoring checkpoint: {checkpoint_path}')\n",
    "    resume_from_checkpoint = checkpoint_path\n",
    "\n",
    "all_data = [X_train, y_train,X_test,y_test]\n",
    "trainer = pl.Trainer(gpus=0,\n",
    "                     max_epochs=max_epochs,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     profiler=True,\n",
    "                     accumulate_grad_batches=accumulate_grad_batches,\n",
    "                     checkpoint_callback=checkpoint_callback,\n",
    "                     progress_bar_refresh_rate=10,\n",
    "                     resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "bert = BertFinetuner(all_data=all_data,hyperparms=hyperparms,sampler = True) \n",
    "\n",
    "trainer.fit(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: You're resuming from a checkpoint that ended mid-epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint. \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339f6ab0ac9a4315b31b1af9ce39a35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização de desempenho\n",
    "\n",
    "A [**Matriz de Confusão**](https://en.wikipedia.org/wiki/Confusion_matrix) (Confusion Matrix) é uma tabela que nos permite a visualização do desempenho de um algoritmo de classificação. <br>\n",
    "É extremamente útil para mensurar [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification), [Recall, Precision, and F-measure](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# uses the model to make predictions on the Test Dataset\n",
    "y_pred = list(bert.df_test['PREDICTED_CODE']) \n",
    "y_prob =  [[a,b] for a,b in zip(list(bert.df_test['INC_PROBA']),list(bert.df_test['REQ_PROBA']))]\n",
    "\n",
    "# computes confusion matrix\n",
    "labels = np.unique(y)\n",
    "data = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# computes precision, recall, f1-score, support (for multiclass classification problem) and accuracy\n",
    "if len(labels) > 2:\n",
    "    # multiclass classification\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_test, y_pred,\n",
    "                                                  labels=labels,\n",
    "                                                  average=None)\n",
    "    \n",
    "    commom_metrics = pd.DataFrame(data=zip(p, r, f1, s),columns=['Precision','Recall','F1-Score','Support']) \n",
    "    \n",
    "    average_options = ('micro', 'macro', 'weighted')\n",
    "    for average in average_options:\n",
    "        if average.startswith('micro'):\n",
    "            line_heading = 'accuracy'\n",
    "        else:\n",
    "            line_heading = average + ' avg'\n",
    "\n",
    "        # compute averages with specified averaging method\n",
    "        avg_p, avg_r, avg_f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, labels=labels,\n",
    "            average=average)\n",
    "        avg = pd.Series({'Precision':avg_p,  'Recall':avg_r,  'F1-Score':avg_f1,  'Support':np.sum(s)},name=line_heading)\n",
    "        commom_metrics = commom_metrics.append(avg)\n",
    "else:\n",
    "    # binary classification\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred,\n",
    "                                                  average='binary')\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    commom_metrics = pd.DataFrame(data={'Precision':p,'Recall':r,'F1-Score':f1,'Accuracy':accuracy},index=[1])\n",
    "\n",
    "# puts matrix in pandas.DataFrame for better format\n",
    "labels = label_encoder.inverse_transform(labels)\n",
    "confusion_matrix = pd.DataFrame(data, columns=labels, index=labels)\n",
    "\n",
    "# add correct index labels to commom_metrics DataFrame (for multiclass classification)\n",
    "if len(labels)>2:\n",
    "    as_list = commom_metrics.index.tolist()\n",
    "    as_list[0:len(labels)] = labels\n",
    "    commom_metrics.index = as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva métricas\n",
    "\n",
    "Utiliza a função `save_metrics` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para salvar métricas. Por exemplo: `accuracy`, `precision`, `r2_score`, `custom_score` etc.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_metrics\n",
    "\n",
    "save_metrics(confusion_matrix=confusion_matrix,commom_metrics=commom_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva figuras\n",
    "\n",
    "Utiliza a função `save_figures` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para salvar figuras do [matplotlib](https://matplotlib.org/3.2.1/gallery/index.html). <br>\n",
    "\n",
    "A avaliação do desempenho do modelo pode ser feita por meio da análise da [Curva ROC (ROC)](https://pt.wikipedia.org/wiki/Caracter%C3%ADstica_de_Opera%C3%A7%C3%A3o_do_Receptor).  Esse gráfico permite avaliar a performance de um classificador binário para diferentes pontos de cortes. A métrica [AUC (Area under curve)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) também é calculada e indicada na legenda do gráfico.<br>\n",
    "Se a variável resposta tiver mais de duas categorias, o cálculo da curva ROC e AUC é feito utilizando o algoritmo [one-vs-rest](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics), ou seja, calcula-se a curva ROC e AUC de cada classe em relação ao restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "from platiagro import save_figure\n",
    "from platiagro import list_figures\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = list(bert.df_test['PREDICTED_CODE']) \n",
    "y_prob =  [[a,b] for a,b in zip(list(bert.df_test['INC_PROBA']),list(bert.df_test['REQ_PROBA']))]\n",
    "\n",
    "def plot_roc_curve(y_test,y_prob,labels):\n",
    "    n_classes = len(labels)\n",
    "    y_test = list(y_test)\n",
    "    y_prob=np.array(y_prob)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        # Compute ROC curve \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)  \n",
    "        \n",
    "        # Plot ROC Curve\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([-0.01, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        save_figure(figure=plt.gcf())\n",
    "        plt.show()\n",
    "        \n",
    "    else:  \n",
    "        # Binarize the output\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        y_test_bin = lb.fit_transform(y_test)\n",
    "\n",
    "        # Compute ROC curve for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()  \n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        color=cm.rainbow(np.linspace(0,1,n_classes+1))\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([-0.01, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        \n",
    "        for i,c in zip(range(n_classes),color):                   \n",
    "            plt.plot(fpr[i], tpr[i], color=c,\n",
    "             lw=lw, label='ROC curve - Class %s (area = %0.2f)' % (labels[i] ,roc_auc[i]))\n",
    "            plt.title('ROC Curve One-vs-Rest')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        save_figure(figure=plt.gcf())\n",
    "        plt.show()\n",
    "\n",
    "plot_roc_curve(y_test,y_prob,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva alterações no conjunto de dados\n",
    "\n",
    "O conjunto de dados será salvo (e sobrescrito com as respectivas mudanças) localmente, no container da experimentação, utilizando a função `pandas.DataFrame.to_csv`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "new_columns = np.array(bert.df_test.columns)\n",
    "df_test_x = pd.DataFrame(X_test,columns=['text'])\n",
    "result_dataframe = pd.concat([df_test_x, bert.df_test], axis=1, sort=False)\n",
    "\n",
    "# save dataset changes\n",
    "bert.df_test.to_csv(f'/tmp/data/{dataset}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva modelo e outros artefatos\n",
    "\n",
    "Utiliza a função `save_model` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para salvar modelos e outros artefatos.<br>\n",
    "Essa função torna estes artefatos disponíveis para o notebook de implantação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_model\n",
    "\n",
    "save_model(columns=columns,\n",
    "           label_encoder=label_encoder,\n",
    "           model=bert,\n",
    "           method=method,\n",
    "           new_columns=new_columns)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "eff11f89-05a0-48e6-b9b7-4ec7fe533f8e",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "operator_id": "eaa2c39c-2f30-4e8a-86fd-d9763448ef04"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
